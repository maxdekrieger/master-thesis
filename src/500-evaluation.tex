% !TEX root = document.tex

\chapter{\label{chap:evaluation}Evaluation}

In this chapter we evaluate the newly introduced SDF3 and Statix specifications for WebDSL. The new specifications have two concrete use-cases, namely serving as a case study for SDF3 and Statix, and being used on a daily basis by WebDSL developers. For both purposes it is useful to gather information about how the specifications behave in various situations. As a result of the case study, we want to show strengths and weaknesses of SDF3 and Statix based on information from the specifications, and for the WebDSL developers we would like to decide whether the new specifications are ready to be used in practice.

For both specifications, we will evaluate their correctness and performance on existing test suites, as well as WebDSL code that is used in practice. Then, we conclude this chapter by discussing the usability of the modernized implementation in practice.

\section{\label{sec:eval-sdf3}Evaluating the WebDSL SDF3 Specification}

  Evaluating the SDF3 specification of the WebDSL grammar is done in two parts: its correctness and its performance in terms of generated parse tables and their run time. In this section we will use the current implementation of the WebDSL grammar in SDF2 as the reference grammar for correctness and performance.

  \subsection{Correctness}

    In this thesis we do not formally prove the correctness of the new grammar. Instead, we parse test suites that are intended for the current SDF2 specification and parse open source WebDSL applications that are used in practice.

    The test suite consists of 231 WebDSL snippets, ranging from single expressions to complete functioning applications. To re-use this test suite for the SDF3 specification, we converted the snippets into SPT tests and the result is that all of the 231 snippets parse succesfully.

    Upon closer inspection of the original test suite, while converting it to an SPT test suite, we concluded it was not a complete test suite of all syntax constructs but mostly contained syntax fragments which were problematic in the past to serve as a regression test suite. For the sake of completion, we decided to extend the SPT test suite, leading to a new total of 1118 SPT tests, where the newly added test have an expected AST result, instead of only expecting the snippets to parse correctly.

    In addition to the test suite, we used two open-source WebDSL applications for verifying that the new parser generated from the SDF3 specification does not suddenly fail or see ambiguities in existing applications:

    \begin{itemize}
      \item \textbf{Reposearch:} A source code search engine that helps to find implementation details, example usages, etc. Reposearch consists of 16 main files, 19 library files and 1 standard library file, totalling at 8.722 lines of code spread over 36 files.
      \item \textbf{YellowGrass:} A tag based issue tracker similar to GitHub Issues, complete with access control and used daily by WebDSL developers. YellowGrass consists of 54 WebDSL files plus 20 WebDSL library files and 1 standard library file, coming to a total of 12.898 lines of code spread over 75 files.
    \end{itemize}

    The result of parsing both applications with the new parser generated with the SDF3 specification is that no ambiguities were found.

    One thing to note in discussing correctness of the WebDSL SDF3 completeness is that, while the results are promising, the SDF3 specification has introduced many new sorts and constructors for disambiguation purposes, and to comply with the Statix Signature Generator expectations. The effect of this change is that the resulting ASTs were not compared and we can therefore not guarantee correctness of the disambiguation, other than the subjective confidence gained from handpicking snippets and comparing the ASTs manually.

  \subsection{Performance}

    The performance of a parser of a programming language is essential due to the rest of the compilation chain depending on its output. A requirement to use the parser generated by the new SDF3 specification in practice, is that its run time should not increase substantially.

    Grammar specifications in SDF2 and SDF3 are not interpreted directly. Both formalisms generate a parse table, which is interpreted by the parser implementation JSGLR. JSGLR is an implementation of SGLR parsing in Java, used within the Spoofax Language Workbench. Because of this architecture, it is insightful to inspect the generated parse tables and highlight the differences, as well as comparing the run times of both parsers on the test suite and existing applications.

    \begin{table}[h]
      \centering
      \begin{tabular}{ |c||c | c | p{2cm} | c | p{2.2cm}| }
        \hline
        Parse table from & States & Gotos & Max gotos per state & Actions & Max actions per state \\
        \hline\hline
        SDF2 & 10.449 & 179.454 & 510 & 62.127 & 107 \\
        \hline
        SDF3 & 12.866 & 244.688 & 821 & 525.728 & 2.491 \\
        \hline
      \end{tabular}
      \caption{\label{tbl:parse-table-differences}Data about the size of the parse tables generated from the SDF2 and SDF3 grammar specifications.}
    \end{table}

    The parse table generated from the SDF3 specification has more states, gotos and actions than the parse table from the SDF2 specification. Even though the described grammar did not change, it is implemented differently, leading to the increase in parse table size. To see the impact of the larger parse table on the run time, we executed the evaluation on Reposearch, YellowGrass and all files in the analysis test suite, which contains complete WebDSL programs as opposed to the syntax test suite. The analysis test suite consists of 521 small files, with a total of 19.644 lines of code.

    To execute the evaluation, we used a 2019 MacBook Pro running macOS Montery 12.2. The machine has a 2,3 GHz 8-core Intel Core i9 with 64 GB RAM available. The evaluation scripts\footnote{https://github.com/metaborg/jsglr2evaluation} were configured to parse the described files with the SDF2 parse table, as well as the SDF3 parse table using the JSGLR1 parser implementation. Using the Java Microbenchmark Harness\footnote{https://github.com/openjdk/jmh}, we timed the run time of the parsers using 5 warmup iterations and 10 regular iterations.

    \begin{figure}
      \pgfplotstableread{
   	    size  sdf2  sdf3
        0     317   377
    	  1     362   467
    	  2     493   538
      }\parsingbenchmarkresults
      \begin{tikzpicture}
        \begin{axis} [ybar,
            axis on top,
            width=\textwidth,
            title={Parser Benchmark Results},
            legend style={
              legend columns=2,
              at={(xticklabel cs:0.5)},
              anchor=north,
              draw=none
            },
            bar width=30,
            ymin=0,
            ylabel={Run time (ms)},
            nodes near coords,
            xtick=data,
            xticklabels={Reposearch, YellowGrass, Analysis Test Suite},
            enlarge x limits=0.3
            ]
        
          \addplot table[x=size, y=sdf2] {\parsingbenchmarkresults};
          \addplot table[x=size, y=sdf3] {\parsingbenchmarkresults};
          \legend{SDF2, SDF3}
        \end{axis}
      \end{tikzpicture}
      \caption{\label{fig:syntax-performance-charts}Run time of SDF2 definition vs. SDF3 definition}
    \end{figure}

    The result of benchmarking the run time of the syntax definitions is shown in \cref{fig:syntax-performance-charts}. Similar to the growth of the parse table generated from the SDF3 definition, the run time has also increased, as was to be expected.

  \subsection{Maintainability}

    Two of the goals of introducing SDF3 as successor of SDF2 in the syntax formalism family, were to support more declarative syntax definition and to make the syntax definitions more readable and understandable \autocite{AmorimV20}. \Cref{fig:sdf3-maintainability-1} shows snippets of the SDF2 and SDF3 specifications that define a WebDSL for-loop. The snippets show that the SDF3 syntax is more similar to other grammar formalisms such as EBNF \autocite{BackusBGK0NPRSV63} and for this reason we argue that the WebDSL syntax definition in SDF3 is easier to read and understand than its predecessor in SDF2.

    \begin{figure}
      \begin{subfigure}[b]{1\textwidth}
        \begin{minted}[firstline=3]{\sdftwo}
module WebDSL-UI
  exports
  context-free syntax

    "for" "(" Id ":" Sort "in" Exp OptFilter ")"
      "{" TemplateElement* "}" ForSeparator -> TemplateElement {cons("For")}

    "separated-by" "{" TemplateElement* "}" -> ForSeparator{cons("ForSeparator")}
                                            -> ForSeparator{cons("None")}
        \end{minted}
        \caption{\label{fig:sdf3-maintainability-1-sdf2}}
      \end{subfigure}
      \begin{subfigure}[b]{1\textwidth}
        \begin{minted}[firstline=2]{\sdfthree}
module WebDSL-UI
  context-free sorts

    TemplateElement ForSeparator

  context-free syntax

    TemplateElement.For = <
      for ( <VarId> : <Sort> in <Exp> <OptFilter> ) {
        <TemplateElement*>
      } <ForSeparator>
    >

    ForSeparator.ForSeparator     = <separated-by { <TemplateElement*> }>
    ForSeparator.ForSeparatorNone = <>
        \end{minted}
        \caption{\label{fig:sdf3-maintainability-1-sdf3}}
      \end{subfigure}
      \caption{\label{fig:sdf3-maintainability-1}Defining a WebDSL for-loop in SDF2 and SDF3}
    \end{figure}

    However, being easier to read and understand does not automatically make the new syntax definition easier to maintain. The compliance with the Statix Signature Generator\footnote{https://www.spoofax.dev/howtos/statix/signature-generator/} imposes constraints on the grammar, such as disallowing optional sorts, which in the worst case causes the amount of sorts in the grammar to double as described in \cref{subsubsec:sdf3-optional-sorts}. Additionally, disambiguation without the \texttt{prefer} and \texttt{avoid} keywords, as described in \cref{sec:webdsl-sdf3-disambiguation}, creates the need for even more sorts which artificially complicate the grammar definition.

\section{\label{sec:eval-statix}Statix}

  Static consistency checking through static analysis is one of the core aspects of WebDSL \autocite{Hemel2011}. However, since no formal semantics of WebDSL are described, we rely on the current implementation of the static analysis in Stratego as the ground thruth.
  
  The evaluation of the Statix specification of WebDSL consists of three parts. First, we list results on the correctness; whether the static analysis allows well-formed programs to pass and whether it gives the correct feedback for erroneous programs. Next, we evaluate the performance in terms of run time on the applications Reposearch and YellowGrass. Lastly, we make qualitative observations on the maintainability of the new Statix specifications.

  \subsection{Correctness}

    One of the goals of analyzing source code before compiling and running it, is to provide early feedback to the developer regarding possible errors. The range of errors that can be caught early is extensive in WebDSL compared to other programming languages, because of the linguistic integration of user interfaces, request handling, access control and the data model.

    For evaluating the correctness of the Statix specification, we first run the static analysis on Reposearch and YellowGrass. Both applications are well-formed programs without errors, therefore the desired result of the static analysis is to report no errors.

    \begin{table}[h]
      \centering
      \begin{tabular}{ | c || c | c | c | }
        \hline
        Project & Files & Lines of code & Errors \\
        \hline
        Reposearch & 36 & 8.722 & 4 \\
        \hline
        Yellowgrass & 75 & 12.898 & 6 \\
        \hline
      \end{tabular}
      \caption{\label{tbl:statix-reposearch-yellowgrass-results}Results of running the static analysis on Reposearch and Yellowgras.}
    \end{table}

    \begin{itemize}
      \item[TO-DO:]
      \item Discuss results
    \end{itemize}

    It is trivial to write a program that does not analyse anything and therefore never give an error, and it would technically suit our goal of analysing Reposearch and YellowGrass without errors. To make sure the Statix specification gives feedback when it encounters an incorrect program, we run the Statix static analysis on the analysis test suite of the current implementation in Stratego. In total, the test suite consists of 521 small programs, testing different aspects of the WebDSL language. 273 files contain a correct program and expect the analysis to give no errors, while 248 programs contain in incorrect program where the static analysis must give specific feedback. The expectation as in those 248 files are listed as first lines in the file as comments, and can be one or more of the following.

    \begin{itemize}
      \item Should give an error containing the message $s$, denoted by \texttt{//s}.
      \item Should give an error containing the message $s$ exactly $x$ times, denoted by \texttt{//\#x s}.
      \item Should not show an error with message $s$, denoted by \texttt{//\^{}s}.
    \end{itemize}

    The results of running the Statix specification on the analysis test suite is shown in \cref{tbl:statix-test-suite-results} below.

    \begin{table}[h]
      \centering
      \begin{tabular}{ | c || c | c | }
        \hline
        & Test succeeded & Test failed \\
        \hline
        Correct programs & 231 & 42 \\
        \hline
        Incorrect programs & 71 & 177 \\
        \hline\hline
        Total & 302 & 219 \\
        \hline
      \end{tabular}
      \caption{\label{tbl:statix-test-suite-results}Results of the analysis test suite.}
    \end{table}

    The result of the analysis test suite is the most variable of all results, as it is an ongoing process, as tweaking error messages and giving more specific error messages in certain circumstances is time consuming. At the time of writing, the Git repository containing implementation of the static analysis has over 400 commits and more tests are passing each week.

    \begin{itemize}
      \item[TO-DO:]
      \item Discuss results more
    \end{itemize}

  \subsection{Performance}

    \begin{itemize}
      \item Analyze analysis test suite files with both Stratego and Statix analysis, show difference in timings
      \item Analyze Reposearch and YellowGrass with both Stratego and Statix analysis, show difference in timings
    \end{itemize}

  \subsection{Maintainability}

    \begin{itemize}
      \item Lack of user-friendliness of the error messages generated by Statix
      \item Can the WebDSL Statix specification be used as formal specification?
      \item Maintainability of the codebase
    \end{itemize}
